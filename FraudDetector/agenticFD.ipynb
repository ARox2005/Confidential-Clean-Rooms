{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32afd8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=11>\n",
      ") candidates=[Candidate(\n",
      "  content=Content(\n",
      "    parts=[\n",
      "      Part(\n",
      "        text=\"\"\"**Trading Anomalies:**\n",
      "*   Stock: GME, Timestamp: 2021-01-20 09:33:15.790837248-05:00, Trade Volume: 2607556, Anonymized Trader ID: user-h3i8\n",
      "*   Stock: GME, Timestamp: 2021-01-20 16:14:20.841945856-05:00, Trade Volume: 3171136, Anonymized Trader ID: user-d9f2\n",
      "*   Stock: GME, Timestamp: 2021-01-20 17:34:52.934853120-05:00, Trade Volume: 1702850, Anonymized Trader ID: user-d9f2\n",
      "\n",
      "**Hype Events:**\n",
      "*   Stock: GME, Timestamp: 2021-01-20 16:39:31.450308864-05:00, Activity Level: 144016\n",
      "\n",
      "**Correlation Events:**\n",
      "No correlated events found where a trading anomaly for a specific stock occurs within 15 minutes *after* a hype event for the same stock.\"\"\"\n",
      "      ),\n",
      "    ],\n",
      "    role='model'\n",
      "  ),\n",
      "  finish_reason=<FinishReason.STOP: 'STOP'>,\n",
      "  index=0\n",
      ")] create_time=None model_version='gemini-2.5-flash' prompt_feedback=None response_id='D2C-aLSmAcCmmtkPvdKX-QI' usage_metadata=GenerateContentResponseUsageMetadata(\n",
      "  candidates_token_count=316,\n",
      "  prompt_token_count=7327,\n",
      "  prompt_tokens_details=[\n",
      "    ModalityTokenCount(\n",
      "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
      "      token_count=7327\n",
      "    ),\n",
      "  ],\n",
      "  thoughts_token_count=3718,\n",
      "  total_token_count=11361\n",
      ") automatic_function_calling_history=[] parsed=None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type GenerateContentResponse is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 80\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# alerts_json = response.to_dict()\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mresult.json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# for event in stream:\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m#     # Every event has candidates (model responses)\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m#     for cand in event.candidates or []:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     86\u001b[39m \u001b[38;5;66;03m#                 if hasattr(part, \"text\") and part.text:\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m#                     print(part.text, end=\"\", flush=True)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\json\\__init__.py:179\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[39m\n\u001b[32m    173\u001b[39m     iterable = \u001b[38;5;28mcls\u001b[39m(skipkeys=skipkeys, ensure_ascii=ensure_ascii,\n\u001b[32m    174\u001b[39m         check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n\u001b[32m    175\u001b[39m         separators=separators,\n\u001b[32m    176\u001b[39m         default=default, sort_keys=sort_keys, **kw).iterencode(obj)\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\json\\encoder.py:440\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    438\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCircular reference detected\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    439\u001b[39m     markers[markerid] = o\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m o = \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Python\\Lib\\json\\encoder.py:180\u001b[39m, in \u001b[36mJSONEncoder.default\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[32m    162\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m \n\u001b[32m    179\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    181\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mis not JSON serializable\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Object of type GenerateContentResponse is not JSON serializable"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Configure the API key\n",
    "client = genai.Client(api_key=\"\")\n",
    "\n",
    "# Load your datasets into strings\n",
    "try:\n",
    "    broker_a_data = pd.read_csv(\"BrokerA_dataset.csv\").to_csv(index=False)\n",
    "    broker_b_data = pd.read_csv(\"BrokerB_dataset.csv\").to_csv(index=False)\n",
    "    social_data = pd.read_csv(\"SocialPlatformX_dataset.csv\").to_csv(index=False)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Make sure all three CSV files are in the same directory.\")\n",
    "    exit()\n",
    "\n",
    "# The prompt is the key â€“ see the next section for the full prompt text\n",
    "prompt_template = \"\"\"\n",
    "You are an expert financial fraud analyst. Your task is to detect pump-and-dump schemes by analyzing trading and social media data.\n",
    "\n",
    "You will be given three datasets in CSV format:\n",
    "1.  **BrokerA_dataset.csv**: Trading data from Broker A, including an anonymized_trader_id.\n",
    "2.  **BrokerB_dataset.csv**: Trading data from Broker B, including an anonymized_trader_id.\n",
    "3.  **SocialPlatformX_dataset.csv**: Social media activity data.\n",
    "\n",
    "**Follow these steps precisely:**\n",
    "\n",
    "1.  **Combine Data**: Mentally combine the trading data from Broker A and Broker B.\n",
    "2.  **Identify Social Hype**: Scan the social media data for timestamps where the 'activity_level' is greater than 30000 for a stock. Note these as \"hype events.\"\n",
    "3.  **Identify Trading Anomalies**: Scan the combined trading data for timestamps where the 'trade_volume' is greater than 500000. Note these as \"trading anomalies.\"\n",
    "4.  **Correlate Events**: Find instances where a \"trading anomaly\" for a specific stock occurs within 15 minutes *after* a \"hype event\" for the same stock.\n",
    "5.  **Generate Alerts**: For each correlated event you find, generate a detailed alert. Identify the `anonymized_trader_id`s involved in the anomalous trades.\n",
    "\n",
    "**Output Format:**\n",
    "List each trading anomalies\n",
    "List each hype event\n",
    "List each correlation event if exist.\n",
    "Do not include introductory remarks or topics, just give the findings in continuous text.\n",
    "\n",
    "---\n",
    "**DATASETS:**\n",
    "\n",
    "**BrokerA_dataset.csv:**\n",
    "```csv\n",
    "{broker_a_csv}\n",
    "\n",
    "**BrokerB_dataset.csv:**\n",
    "```csv\n",
    "{broker_b_csv}\n",
    "\n",
    "**SocialPlatformX_dataset.csv:**\n",
    "```csv\n",
    "{social_csv}\n",
    "\"\"\"\n",
    "\n",
    "# Insert the data into the prompt\n",
    "full_prompt = prompt_template.format(\n",
    "    broker_a_csv=broker_a_data[:2500],\n",
    "    broker_b_csv=broker_b_data[:2500],\n",
    "    social_csv=social_data[:2500]\n",
    ")\n",
    "\n",
    "# Call the Gemini API\n",
    "# model = genai.GenerativeModel('gemini-2.5-flash')\n",
    "# response = model.generate_content(full_prompt)\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=full_prompt\n",
    ")\n",
    "# stream = client.models.generate_content_stream(\n",
    "    # model=\"gemini-2.5-flash\",\n",
    "    # contents=full_prompt,\n",
    "# )\n",
    "\n",
    "# Print the model's analysis\n",
    "print(response)\n",
    "# alerts_json = response.to_dict()\n",
    "with open(\"result.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(response, f, indent=4)\n",
    "# for event in stream:\n",
    "#     # Every event has candidates (model responses)\n",
    "#     for cand in event.candidates or []:\n",
    "#         if cand.content and cand.content.parts:\n",
    "#             for part in cand.content.parts:\n",
    "#                 if hasattr(part, \"text\") and part.text:\n",
    "#                     print(part.text, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
